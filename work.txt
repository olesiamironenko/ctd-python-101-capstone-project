1. Scraping

    Scraping 1 year results page: 
        Get years and links from 'Year to Year' page
        get full html from div.container
            parse container
                parse each table in container
                - td.header, formated -> csv file name,
                    - if exist: open to append,
                    - if does not exist: create and write
                - td.banner -> csv fields (1st row),
                - 'td' calas include 'datacol' -> csv rows

        Years table: 
            - table class=ba-sub

        Year links:
            - a
            - href, relative link
            - title, contain year and league name
            - text, contain year


    to create csv file name:
    - get prefix from page url
    - get slugified text from td.header
    - add .csv

    create csv file name:
        prefix + slugified text + .csv


    Teams and players

    - get teams from the table whith the header Major League only:
        - get td.header, first only,
        - get td.banner, first tr only, create 2 dicts for collecting team names and links,
        - get team names and links from trs:
            - first td goes to first banner list   
            - second td goes to second bannner list

    - iterate through the teams roster links to get ("roster" is part of the link title):
        - scrape tb.boxed and form rows for futre csv
            - get td.banner
            - get td containing "datacol"
            - add team name as the first row (team name is the title from roster_links list)
        - write/append csv
            - name csv based on text in div.navbaractive
            - write/append rows

        - go to next link from div.navbaractive
            - scrape tb.boxed and form rows for futre csv
                - get td.banner
                - get td containing "datacol"
                - add team name as the first row
            - write/append csv
                - name csv based on text in div.navbaractive
                - write/append rows

1.5 Data cleaning
    - load csv to df 
    - drop nas and duplicates
    
    - separate big dfs into smaler ones,
    - connect dfs using pk fk connection


    